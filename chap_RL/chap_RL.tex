%!TEX root = ../thesis_main.tex
%!TEX encoding = UTF-8 Unicode

\section{Definition of the actions, states and rewards}
\label{sec:act_states_rew}
After pointing out that the environment is basically the EnergyScope myopic model at the different steps of the transition, define the actions, the states as well as the shape of the reward.

\subsection{Actions}
\subsection{States}
\subsection{Reward}

\section{Results - As in Smart Energy Systems Conference}

%\subsection{Training}
%Here present results RL-oriented (actions, states, reward)
%
%Then, present results energy system-oriented (installed capacities, costs, generation)

%\subsection{Testing}
%See how the different policies saved successively during the training behave when facing to new samples. This way, we can pick an optimal policy (the one giving the maximum average (or another metric) reward). 
%
%We then compare the results of RL with the perfect foresight-TD deterministic and the perfect foresight-monthly with uncertainties (by setting the actions of the agent as variables in these two optimisations). The comparison would go over different aspects:
%\begin{itemize}
%\item Over-cost
%\item Over-change, a bit like what Paolo defined as design error/change (in terms in consumed resources, installed capacities,...)
%\end{itemize}


